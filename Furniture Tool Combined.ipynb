{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmP4zKd9OmGy"
      },
      "source": [
        "**Segment anything - deleting from an image - SAM, inpaiting, resizing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zAbbNT6xWJM",
        "outputId": "a706d355-c73d-4efc-f1c8-44ca734483b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simple-lama-inpainting in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: segment-anything in /usr/local/lib/python3.10/dist-packages (1.0)\n",
            "Requirement already satisfied: fire<0.6.0,>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from simple-lama-inpainting) (0.5.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from simple-lama-inpainting) (1.25.2)\n",
            "Requirement already satisfied: opencv-python<5.0.0.0,>=4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from simple-lama-inpainting) (4.8.0.76)\n",
            "Requirement already satisfied: pillow<10.0.0,>=9.5.0 in /usr/local/lib/python3.10/dist-packages (from simple-lama-inpainting) (9.5.0)\n",
            "Requirement already satisfied: torch!=2.0.1,>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from simple-lama-inpainting) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from simple-lama-inpainting) (0.17.1+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire<0.6.0,>=0.5.0->simple-lama-inpainting) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire<0.6.0,>=0.5.0->simple-lama-inpainting) (2.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=2.0.1,>=1.13.1->simple-lama-inpainting) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: segment-anything in /usr/local/lib/python3.10/dist-packages (1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install simple-lama-inpainting segment-anything\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "%reload_ext autoreload\n",
        "!pip install segment-anything"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fp4pMnZpwh3L"
      },
      "outputs": [],
      "source": [
        "#!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dLKBQHyhzq1E"
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "from simple_lama_inpainting import SimpleLama\n",
        "\n",
        "import scipy\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.interpolate import griddata\n",
        "\n",
        "# modify to where you store your project data including utils\n",
        "datadir = \"CS445_Group_Project/Project_Files\"\n",
        "\n",
        "# can change this to your output directory of choice\n",
        "!mkdir \"images\"\n",
        "!mkdir \"images/outputs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "fr95f-pzxmrL",
        "outputId": "4a577766-0f85-463e-b7c5-8f18d7349321"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Manso\\\\CS445_Group_Project\\\\sam_vit_h_4b8939 (2).pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-430653aef90c>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msam_model_registry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msam_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0msam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py\u001b[0m in \u001b[0;36mbuild_sam_vit_h\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_sam_vit_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     return _build_sam(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mencoder_embed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mencoder_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py\u001b[0m in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0msam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0msam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Manso\\\\CS445_Group_Project\\\\sam_vit_h_4b8939 (2).pth'"
          ]
        }
      ],
      "source": [
        "from segment_anything import sam_model_registry\n",
        "\n",
        "# Path to the model checkpoint\n",
        "sam_checkpoint = sam_checkpoint = \"C:\\\\Users\\\\Manso\\\\CS445_Group_Project\\\\sam_vit_h_4b8939 (2).pth\"\n",
        "\n",
        "model_type = \"vit_h\"\n",
        "\n",
        "# Check if CUDA is available, otherwise use CPU\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = \"cpu\"\n",
        "\n",
        "# Load the model\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "print(\"Loaded on\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "MFWkGhLHFtvc",
        "outputId": "3a33cd06-250e-4c3f-b7af-e8c04bf0fecb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Project_Files\\\\room.jpeg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f6eb86ccbdba>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"Project_Files\\room.jpeg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Project_Files\\\\room.jpeg'"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image_path = r\"Project_Files\\room.jpeg\"\n",
        "image = Image.open(image_path)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.axis('off')  # Hide axes ticks if you want\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7o82ChFsYRN2"
      },
      "outputs": [],
      "source": [
        "room4 = r\"Project_Files\\room4.jpg\"\n",
        "#image_path = \"/content/final/room.jpeg\"\n",
        "\n",
        "room4 = Image.open(room4)\n",
        "\n",
        "plt.imshow(room4)\n",
        "# plt.axis('off')  # Hide axes ticks if you want\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5AczEaO8q_I"
      },
      "outputs": [],
      "source": [
        "couch10_path = r\"Project_Files\\better_sofa.jpg\"\n",
        "\n",
        "couch10 = Image.open(couch10_path)\n",
        "\n",
        "plt.imshow(couch10)\n",
        "# plt.axis('off')  # Hide axes ticks if you want\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk35MraVxtfd"
      },
      "outputs": [],
      "source": [
        "#room.jpeg\n",
        "width, height = image.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV8t7gLxYYpJ"
      },
      "outputs": [],
      "source": [
        "#room4.jpg\n",
        "width_room4, height_room4 = room4.size\n",
        "widthc, heightc = room4.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn_k0Jnr9OBo"
      },
      "outputs": [],
      "source": [
        "#better_sofa.jpg\n",
        "width_c10, height_c10 = couch10.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wFYm5Q_DxwTa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "93bb6338-bde0-4d70-9e7c-40d79c346de0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'width' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1ce8e27658bf>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m520\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m130\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnormalized_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpoint_grids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormalized_point\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'width' is not defined"
          ]
        }
      ],
      "source": [
        "#room.jpeg\n",
        "x, y = 520, 130\n",
        "\n",
        "normalized_point = np.array([[x / width, y / height]])\n",
        "\n",
        "point_grids = [normalized_point]\n",
        "\n",
        "from segment_anything import SamAutomaticMaskGenerator\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=None,\n",
        "    point_grids=point_grids,\n",
        "    pred_iou_thresh=0.5,  # Lowered from 0.88\n",
        "    stability_score_thresh=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHN1yMPYYsZs"
      },
      "outputs": [],
      "source": [
        "#room4.jpg\n",
        "xc, yc = 535, 140\n",
        "\n",
        "normalized_point_room4 = np.array([[xc / widthc, yc / heightc]])\n",
        "\n",
        "point_grids_room4 = [normalized_point_room4]\n",
        "\n",
        "mask_generator_room4 = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=None,\n",
        "    point_grids=point_grids_room4,\n",
        "    pred_iou_thresh=0.5,  # Lowered from 0.88\n",
        "    stability_score_thresh=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Pgrb_e0A8s"
      },
      "outputs": [],
      "source": [
        "#room4.jpg\n",
        "xt, yt = 300, 445\n",
        "\n",
        "normalized_point_room4_table = np.array([[xt / widthc, yt / heightc]])\n",
        "\n",
        "point_grids_room4_table = [normalized_point_room4_table]\n",
        "\n",
        "mask_generator_room4_table = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=None,\n",
        "    point_grids=point_grids_room4_table,\n",
        "    pred_iou_thresh=0.5,  # Lowered from 0.88\n",
        "    stability_score_thresh=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1VWRgUiP9VgC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4bd13dfc-e832-48e8-a64f-f0128b1e4e33"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'width_c10' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fa67ef8d183e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnormalized_point_c10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx10\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mwidth_c10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my10\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mheight_c10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpoint_grids_c10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormalized_point_c10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'width_c10' is not defined"
          ]
        }
      ],
      "source": [
        "#better_sofa.jpg\n",
        "x10, y10 = 250, 250\n",
        "\n",
        "normalized_point_c10 = np.array([[x10 / width_c10, y10 / height_c10]])\n",
        "\n",
        "point_grids_c10 = [normalized_point_c10]\n",
        "\n",
        "mask_generator_c10 = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=None,\n",
        "    point_grids=point_grids_c10,\n",
        "    pred_iou_thresh=0.5,  # Lowered from 0.88\n",
        "    stability_score_thresh=0.8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOxEFbrqHZrZ"
      },
      "outputs": [],
      "source": [
        "#room.jpeg\n",
        "image_array = np.array(image)\n",
        "\n",
        "masks = mask_generator.generate(image_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H3f8NITY-W7"
      },
      "outputs": [],
      "source": [
        "#room4.jpg\n",
        "room4_array = np.array(room4)\n",
        "\n",
        "masksroom4 = mask_generator_room4.generate(room4_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sAgdXM09xPN"
      },
      "outputs": [],
      "source": [
        "#better_sofa.jpg\n",
        "couch10_array = np.array(couch10)\n",
        "\n",
        "masks_couch10 = mask_generator_c10.generate(couch10_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCgrmsoFyAhP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, len(masks) + 1, 1)\n",
        "plt.imshow(np.array(image))\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "for i, mask_info in enumerate(masks):\n",
        "    mask = mask_info['segmentation']\n",
        "    if isinstance(mask, dict):\n",
        "        mask = mask_utils.decode(mask)\n",
        "    plt.subplot(1, len(masks) + 1, i + 2)\n",
        "    plt.imshow(np.array(mask), cmap='gray')\n",
        "    plt.title(f'Mask {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjPWkT_8ZIBj"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, len(masksroom4) + 1, 1)\n",
        "plt.imshow(np.array(room4))\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "for i, mask_info in enumerate(masksroom4):\n",
        "    mask = mask_info['segmentation']\n",
        "    if isinstance(mask, dict):\n",
        "        mask = mask_utils.decode(masksroom4)\n",
        "    plt.subplot(1, len(masksroom4) + 1, i + 2)\n",
        "    plt.imshow(np.array(mask), cmap='gray')\n",
        "    plt.title(f'Mask {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QMohQuZ-g3k"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, len(masks_couch10) + 1, 1)\n",
        "plt.imshow(np.array(couch10))\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "for i, mask_info in enumerate(masks_couch10):\n",
        "    mask = mask_info['segmentation']\n",
        "    if isinstance(mask, dict):\n",
        "        mask = mask_utils.decode(mask)\n",
        "    plt.subplot(1, len(masks_couch10) + 1, i + 2)\n",
        "    plt.imshow(np.array(mask), cmap='gray')\n",
        "    plt.title(f'Mask {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzKyOf6jyE9R"
      },
      "outputs": [],
      "source": [
        "plt.imshow(masks_couch10[2]['segmentation'])\n",
        "plt.show()\n",
        "print(masks_couch10[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRv1Iz2SyHq3"
      },
      "outputs": [],
      "source": [
        "image = image\n",
        "mask = masks[1]['segmentation']\n",
        "print(mask)\n",
        "plt.imshow(mask)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izNQzPrGhYSH"
      },
      "outputs": [],
      "source": [
        "# couchmask = maskscouch[0]['segmentation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWrc8YUmyJ9G"
      },
      "outputs": [],
      "source": [
        "def expand_mask(mask, expand_size):\n",
        "    height, width = mask.shape\n",
        "    expanded_mask = np.zeros_like(mask)\n",
        "\n",
        "    for y in range(height):\n",
        "        for x in range(width):\n",
        "            if mask[y, x] == 1:  # Check if the current pixel is part of the mask\n",
        "                top = max(0, y - expand_size)\n",
        "                bottom = min(height, y + expand_size + 1)\n",
        "                left = max(0, x - expand_size)\n",
        "                right = min(width, x + expand_size + 1)\n",
        "                expanded_mask[top:bottom, left:right] = 1\n",
        "\n",
        "    return expanded_mask\n",
        "\n",
        "\n",
        "expand_size = 10\n",
        "expanded_mask = expand_mask(mask, expand_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcfjNPdEyMMs"
      },
      "outputs": [],
      "source": [
        "plt.imshow(expanded_mask)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oCmBtuRyRyN"
      },
      "outputs": [],
      "source": [
        "simple_lama = SimpleLama()\n",
        "\n",
        "image_array = np.array(image)\n",
        "\n",
        "if image_array.dtype != np.uint8:\n",
        "    image_array = (image_array * 255).astype(np.uint8)\n",
        "\n",
        "expanded_mask_array = np.array(expanded_mask)\n",
        "if expanded_mask_array.dtype != np.uint8:\n",
        "    expanded_mask_array = (expanded_mask_array * 255).astype(np.uint8)\n",
        "\n",
        "result = simple_lama(image_array, expanded_mask_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bseEeY9cJ_E8"
      },
      "outputs": [],
      "source": [
        "plt.imshow(result)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmrAnlXyppOs"
      },
      "outputs": [],
      "source": [
        "image_path = r\"Project_Files\\room2.jpg\"\n",
        "\n",
        "image2 = Image.open(image_path)\n",
        "\n",
        "plt.imshow(image2)\n",
        "# plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM7UBYrMwh3s"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'masks' and 'image files' are appropriately defined and available\n",
        "image1 = cv2.imread(r\"Project_Files\\room.jpeg\")\n",
        "image2 = cv2.imread(r\"Project_Files\\room2.jpg\")\n",
        "\n",
        "# Assuming 'masks' is a list of dictionaries, each containing 'segmentation' and 'bbox'\n",
        "mask = masks[1]['segmentation']  # Example mask\n",
        "bbox = masks[1]['bbox']  # Example bounding box\n",
        "\n",
        "cropped_mask = mask[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "\n",
        "cropped_image1 = image1[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
        "scale_factor = 0.5\n",
        "\n",
        "# Resizing the cropped image and mask\n",
        "resized_cropped_image1 = cv2.resize(cropped_image1, (int(cropped_image1.shape[1] * scale_factor), int(cropped_image1.shape[0] * scale_factor)))\n",
        "resized_cropped_mask = cv2.resize(cropped_mask, (int(cropped_mask.shape[1] * scale_factor), int(cropped_mask.shape[0] * scale_factor)))\n",
        "\n",
        "# Creating a masked source image\n",
        "masked_source = cv2.bitwise_and(resized_cropped_image1, resized_cropped_image1, mask=resized_cropped_mask)\n",
        "start_x, start_y = 370, 35\n",
        "end_y = start_y + masked_source.shape[0]\n",
        "end_x = start_x + masked_source.shape[1]\n",
        "\n",
        "# Applying the masked source image to the destination image\n",
        "mask = np.any(masked_source != [0, 0, 0], axis=-1)\n",
        "image2[start_y:end_y, start_x:end_x][mask] = masked_source[mask]\n",
        "\n",
        "# Convert image from BGR to RGB for display in Matplotlib\n",
        "image2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "plt.imshow(image2_rgb)\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.show()\n",
        "\n",
        "output_filename = 'images/outputs/Lamp_without_texture.jpg'\n",
        "cv2.imwrite(output_filename, image2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6a52W0Kwh3s"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load images\n",
        "image1 = cv2.imread(r\"Project_Files\\room4.jpg\")\n",
        "image2 = cv2.imread('images/outputs/Lamp_without_texture.jpg')      #load image with lamp\n",
        "\n",
        "# Assuming maskscouch and bbox information is properly defined somewhere in your script\n",
        "mask_coffee = masksroom4[1]['segmentation']\n",
        "bbox_coffee = masksroom4[1]['bbox']\n",
        "\n",
        "# Process the mask and image for the coffee table\n",
        "cropped_mask = mask_coffee[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "cropped_image = image1[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "\n",
        "# Resize the cropped image and mask\n",
        "scale_factor = 0.5\n",
        "resized_image = cv2.resize(cropped_image, (int(cropped_image.shape[1] * scale_factor), int(cropped_image.shape[0] * scale_factor)))\n",
        "resized_mask = cv2.resize(cropped_mask, (int(cropped_mask.shape[1] * scale_factor), int(cropped_mask.shape[0] * scale_factor)))\n",
        "\n",
        "# Extract the coffee table using the mask\n",
        "table_extracted = cv2.bitwise_and(resized_image, resized_image, mask=resized_mask)\n",
        "\n",
        "# Positioning the extracted table in the second image\n",
        "start_x, start_y = 150, 100\n",
        "end_x = start_x + table_extracted.shape[1]\n",
        "end_y = start_y + table_extracted.shape[0]\n",
        "image2[start_y:end_y, start_x:end_x] = image2[start_y:end_y, start_x:end_x] * (1 - (resized_mask > 0)[:, :, None]) + table_extracted\n",
        "\n",
        "# Show the final image\n",
        "plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Save the final image\n",
        "save_path = 'images/outputs/LampNlight_without_texture.jpg'\n",
        "cv2.imwrite(save_path, image2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozRm7jLxwh3t"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load images\n",
        "image1 = cv2.imread(r\"Project_Files\\better_sofa.jpg\")  # Load the coffee table image\n",
        "image2 = cv2.imread('images/outputs/LampNlight_without_texture.jpg')  # Load the room image\n",
        "\n",
        "mask_coffee = masks_couch10[2]['segmentation']\n",
        "bbox_coffee = masks_couch10[2]['bbox']\n",
        "\n",
        "cropped_mask = mask_coffee[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "cropped_image = image1[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "\n",
        "scale_factor = 0.45\n",
        "resized_image = cv2.resize(cropped_image, (int(cropped_image.shape[1] * scale_factor), int(cropped_image.shape[0] * scale_factor)))\n",
        "resized_mask = cv2.resize(cropped_mask, (int(cropped_mask.shape[1] * scale_factor), int(cropped_mask.shape[0] * scale_factor)))\n",
        "couch_mask = resized_mask\n",
        "\n",
        "table_extracted = cv2.bitwise_and(resized_image, resized_image, mask=resized_mask)\n",
        "\n",
        "\n",
        "start_x, start_y = 220, 220\n",
        "end_x = start_x + table_extracted.shape[1]\n",
        "end_y = start_y + table_extracted.shape[0]\n",
        "\n",
        "start_x_couch, start_y_couch = start_x, start_y\n",
        "\n",
        "image2[start_y:end_y, start_x:end_x] = image2[start_y:end_y, start_x:end_x] * (1 - (resized_mask > 0)[:, :, None]) + table_extracted\n",
        "\n",
        "# Show the final image\n",
        "plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# Save the final image\n",
        "save_path = 'images/outputs/LampNlightNcouch_without_texture.jpg'\n",
        "cv2.imwrite(save_path, image2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cDJara7HkvH"
      },
      "source": [
        "## Texture Transfer using cropped mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9PfRHUcwh3u"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "# from google.colab.patches import cv2_imshow  # Import cv2_imshow to replace cv2.imshow\n",
        "\n",
        "image1 = cv2.imread('Project_Files/room.jpeg')\n",
        "image2 = cv2.imread('Project_Files/room2.jpg')\n",
        "\n",
        "mask = masks[1]['segmentation']\n",
        "bbox = masks[1]['bbox']\n",
        "\n",
        "cropped_mask = mask[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "\n",
        "cropped_image1 = image1[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
        "scale_factor = 0.5\n",
        "\n",
        "resized_cropped_image1 = cv2.resize(cropped_image1, (int(cropped_image1.shape[1] * scale_factor), int(cropped_image1.shape[0] * scale_factor)))\n",
        "resized_cropped_mask = cv2.resize(cropped_mask, (int(cropped_mask.shape[1] * scale_factor), int(cropped_mask.shape[0] * scale_factor)))\n",
        "\n",
        "masked_source = cv2.bitwise_and(resized_cropped_image1, resized_cropped_image1, mask=resized_cropped_mask)\n",
        "start_x, start_y = 370, 35\n",
        "end_y = start_y + masked_source.shape[0]\n",
        "end_x = start_x + masked_source.shape[1]\n",
        "\n",
        "texture_transfer = True\n",
        "\n",
        "if not texture_transfer:\n",
        "    mask = np.any(masked_source != [0, 0, 0], axis=-1)\n",
        "    image2[start_y:end_y, start_x:end_x][mask] = masked_source[mask]\n",
        "else:\n",
        "    texture_image = cv2.imread('Project_Files/texture.png')\n",
        "    texture_image = cv2.resize(texture_image, (masked_source.shape[1], masked_source.shape[0]))\n",
        "    textured_object = cv2.seamlessClone(texture_image, masked_source, resized_cropped_mask, (masked_source.shape[1]//2, masked_source.shape[0]//2), cv2.NORMAL_CLONE)\n",
        "    mask = np.any(textured_object != [0, 0, 0], axis=-1)\n",
        "    image2[start_y:end_y, start_x:end_x][mask] = textured_object[mask]\n",
        "\n",
        "output_filename = 'images/outputs/texture_transfer_lamp.jpg'\n",
        "cv2.imwrite(output_filename, image2)\n",
        "output_image = Image.open('images/outputs/texture_transfer_lamp.jpg')\n",
        "\n",
        "plt.imshow(output_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaDBbFCrwh3v"
      },
      "source": [
        "## Texture Transfer using Poisson Blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhdxPfHwwh3w"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.sparse import lil_matrix, csr_matrix\n",
        "from scipy.sparse.linalg import spsolve\n",
        "\n",
        "def poisson_blend(img_target, img_source, img_mask, offset):\n",
        "    off_y, off_x = offset\n",
        "    y_indices, x_indices = np.nonzero(img_mask)\n",
        "    num_pixels = len(y_indices)\n",
        "    A = lil_matrix((num_pixels, num_pixels), dtype=np.float32)\n",
        "    b = np.zeros((num_pixels, 3), dtype=np.float32)\n",
        "\n",
        "    index_mapping = {idx: i for i, idx in enumerate(zip(y_indices, x_indices))}\n",
        "\n",
        "    for i, (y, x) in enumerate(zip(y_indices, x_indices)):\n",
        "        connections = 0\n",
        "        neighbors = [(y, x - 1), (y, x + 1), (y - 1, x), (y + 1, x)]\n",
        "        for ny, nx in neighbors:\n",
        "            if (ny, nx) in index_mapping:\n",
        "                A[i, index_mapping[(ny, nx)]] = -1\n",
        "                connections += 1\n",
        "            else:\n",
        "                ny_global, nx_global = ny + off_y, nx + off_x\n",
        "                if 0 <= ny_global < img_target.shape[0] and 0 <= nx_global < img_target.shape[1]:\n",
        "                    b[i] += img_target[ny_global, nx_global].astype(np.float32)\n",
        "                else:\n",
        "                    b[i] += img_source[ny, nx].astype(np.float32)\n",
        "        A[i, i] = connections\n",
        "\n",
        "    solution_vectors = np.zeros((num_pixels, 3), dtype=np.float32)\n",
        "    for k in range(3):\n",
        "        solution = spsolve(A.tocsr(), b[:, k])\n",
        "        solution_vectors[:, k] = solution\n",
        "\n",
        "    print(\"Solution range:\", np.min(solution_vectors), np.max(solution_vectors))\n",
        "\n",
        "    # Applying the solution\n",
        "    for idx, (y, x) in enumerate(zip(y_indices, x_indices)):\n",
        "        img_target[y + off_y, x + off_x] = np.clip(solution_vectors[idx], 0, 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "# Read images and mask\n",
        "image1 = cv2.imread('Project_Files/room.jpeg')\n",
        "image2 = cv2.imread('Project_Files/room2.jpg')\n",
        "texture_image = cv2.imread('Project_Files/texture.png')\n",
        "\n",
        "# Segment information\n",
        "mask = masks[1]['segmentation']\n",
        "bbox = masks[1]['bbox']\n",
        "\n",
        "# Preparing the mask and the source area for blending\n",
        "cropped_mask = mask[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "cropped_image1 = image1[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
        "\n",
        "# Resize texture to fit the cropped mask\n",
        "texture_resized = cv2.resize(texture_image, (cropped_image1.shape[1], cropped_image1.shape[0]))\n",
        "\n",
        "# Perform Poisson blending\n",
        "poisson_blend(image2, texture_resized, cropped_mask, (bbox[1], bbox[0]))\n",
        "\n",
        "# Save and display the output\n",
        "output_filename = 'images/outputs/texture_transfer_lamp_blending.jpg'\n",
        "cv2.imwrite(output_filename, image2)\n",
        "output_image = Image.open(output_filename)\n",
        "\n",
        "plt.imshow(output_image)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0WMQhE8wh3x"
      },
      "source": [
        "## Texture Transfer using Alpha Blending"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ibnjMN3wh3x"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def alpha_blend(img_target, img_source, img_mask, offset):\n",
        "    off_y, off_x = offset\n",
        "    for y in range(img_mask.shape[0]):\n",
        "        for x in range(img_mask.shape[1]):\n",
        "            if img_mask[y, x]:\n",
        "                alpha = img_mask[y, x] / 255.0\n",
        "                # Manually blend each channel\n",
        "                for c in range(3):  # Assuming both images are in BGR format\n",
        "                    img_target[off_y + y, off_x + x, c] = (\n",
        "                        img_target[off_y + y, off_x + x, c] * (1 - alpha) +\n",
        "                        img_source[y, x, c] * alpha\n",
        "                    )\n",
        "\n",
        "# Read images and mask\n",
        "image1 = cv2.imread('Project_Files/room.jpeg')\n",
        "image2 = cv2.imread('Project_Files/room2.jpg')\n",
        "texture_image = cv2.imread('Project_Files/texture.png')\n",
        "\n",
        "# Segment information and mask processing\n",
        "mask = masks[1]['segmentation']\n",
        "bbox = masks[1]['bbox']\n",
        "\n",
        "# Prepare the mask and the source area for blending\n",
        "cropped_mask = mask[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "cropped_texture = cv2.resize(texture_image, (cropped_mask.shape[1], cropped_mask.shape[0]))\n",
        "\n",
        "# Perform alpha blending\n",
        "alpha_blend(image2, cropped_texture, cropped_mask, (bbox[1], bbox[0]))\n",
        "\n",
        "# Save and display the output\n",
        "output_filename = 'images/outputs/texture_transfer_lamp_blending.jpg'\n",
        "cv2.imwrite(output_filename, image2)\n",
        "output_image = Image.open(output_filename)\n",
        "\n",
        "plt.imshow(output_image)\n",
        "plt.axis('off')  # Hide axes ticks\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LAGI0Hnwh3y"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def alpha_blend(img_target, img_source, img_mask, offset):\n",
        "    off_y, off_x = offset\n",
        "    for y in range(img_mask.shape[0]):\n",
        "        for x in range(img_mask.shape[1]):\n",
        "            if img_mask[y, x]:\n",
        "                alpha = img_mask[y, x] / 255.0\n",
        "                # Manually blend each channel\n",
        "                for c in range(3):  # Assuming both images are in BGR format\n",
        "                    img_target[off_y + y, off_x + x, c] = (\n",
        "                        img_target[off_y + y, off_x + x, c] * (1 - alpha) +\n",
        "                        img_source[y, x, c] * alpha\n",
        "                    )\n",
        "\n",
        "# Read images and mask\n",
        "image1 = cv2.imread('Project_Files/better_sofa.jpeg')\n",
        "image2 = cv2.imread('images/outputs/texture_transfer_lamp_blending.jpg')\n",
        "texture_image = cv2.imread('Project_Files/texture.png')\n",
        "\n",
        "# Segment information and mask processing\n",
        "mask = masks_couch10[2]['segmentation']\n",
        "bbox = masks_couch10[2]['bbox']\n",
        "\n",
        "# Prepare the mask and the source area for blending\n",
        "cropped_mask = mask[bbox[1]:bbox[1]+bbox[3], bbox[0]:bbox[0]+bbox[2]]\n",
        "\n",
        "cropped_mask = couch_mask\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "cropped_texture = cv2.resize(texture_image, (cropped_mask.shape[1], cropped_mask.shape[0]))\n",
        "\n",
        "# Perform alpha blending\n",
        "alpha_blend(image2, cropped_texture, cropped_mask, (start_x_couch, start_y_couch))\n",
        "\n",
        "# Save and display the output\n",
        "output_filename = 'images/outputs/texture_transfer_couch_blending.jpg'\n",
        "cv2.imwrite(output_filename, image2)\n",
        "output_image = Image.open(output_filename)\n",
        "\n",
        "plt.imshow(output_image)\n",
        "plt.axis('off')  # Hide axes ticks\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERmvDLy-wh3z"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def alpha_blend(img_target, img_source, img_mask, offset):\n",
        "    off_y, off_x = offset\n",
        "    target_height, target_width = img_target.shape[:2]\n",
        "    source_height, source_width = img_source.shape[:2]\n",
        "    mask_height, mask_width = img_mask.shape\n",
        "\n",
        "    for y in range(mask_height):\n",
        "        for x in range(mask_width):\n",
        "            if img_mask[y, x] > 0:\n",
        "                target_y = off_y + y\n",
        "                target_x = off_x + x\n",
        "                if 0 <= target_y < target_height and 0 <= target_x < target_width:\n",
        "                    alpha = img_mask[y, x] / 255.0\n",
        "                    img_target[target_y, target_x] = (\n",
        "                        img_target[target_y, target_x] * (1 - alpha) + img_source[y, x] * alpha\n",
        "                    ).astype(np.uint8)\n",
        "\n",
        "# Read images\n",
        "target_image_path = 'Project_Files/room2.jpg'\n",
        "texture_image_path = 'Project_Files/texture.png'\n",
        "\n",
        "image_target = cv2.imread(target_image_path)\n",
        "texture_image = cv2.imread(texture_image_path)\n",
        "\n",
        "# Load couch mask and adjust if necessary\n",
        "couch_mask = masks_couch10[2]['segmentation']  # Assuming 'masks_couch' is properly defined\n",
        "bbox = masks_couch10[2]['bbox']\n",
        "\n",
        "# Resize texture to fit the mask\n",
        "texture_resized = cv2.resize(texture_image, (couch_mask.shape[1], couch_mask.shape[0]))\n",
        "\n",
        "# Offset calculation based on the bounding box\n",
        "offset = (bbox[1], bbox[0])  # y, x\n",
        "\n",
        "# Perform alpha blending\n",
        "alpha_blend(image_target, texture_resized, couch_mask, offset)\n",
        "\n",
        "# Save and display the output\n",
        "output_filename = 'images/outputs/couch_texture_blend.jpg'\n",
        "cv2.imwrite(output_filename, image_target)\n",
        "output_image = Image.open(output_filename)\n",
        "\n",
        "plt.imshow(output_image)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XciZ-AKmhjl5"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load images\n",
        "image1 = cv2.imread('/Users/prasadgole/Desktop/Illinois/Computational Photography/Group Project/CS445_Group_Project/Project_Files/room4.jpg')  # Load the coffee table image\n",
        "image2 = cv2.imread('/Users/prasadgole/Desktop/Illinois/Computational Photography/Group Project/CS445_Group_Project/Project_Files/room4.jpg')          # Load the room image\n",
        "\n",
        "# Assuming maskscouch and bbox information is properly defined somewhere in your script\n",
        "mask_coffee = maskscouch[1]['segmentation']\n",
        "bbox_coffee = maskscouch[1]['bbox']\n",
        "\n",
        "# Process the mask and image for the coffee table\n",
        "cropped_mask = mask_coffee[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "cropped_image = image1[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "\n",
        "# Resize the cropped image and mask\n",
        "scale_factor = 0.5\n",
        "resized_image = cv2.resize(cropped_image, (int(cropped_image.shape[1] * scale_factor), int(cropped_image.shape[0] * scale_factor)))\n",
        "resized_mask = cv2.resize(cropped_mask, (int(cropped_mask.shape[1] * scale_factor), int(cropped_mask.shape[0] * scale_factor)))\n",
        "\n",
        "# Extract the coffee table using the mask\n",
        "table_extracted = cv2.bitwise_and(resized_image, resized_image, mask=resized_mask)\n",
        "\n",
        "# Positioning the extracted table in the second image\n",
        "start_x, start_y = 150, 100\n",
        "end_x = start_x + table_extracted.shape[1]\n",
        "end_y = start_y + table_extracted.shape[0]\n",
        "image2[start_y:end_y, start_x:end_x] = image2[start_y:end_y, start_x:end_x] * (1 - (resized_mask > 0)[:, :, None]) + table_extracted\n",
        "\n",
        "# Show the final image\n",
        "# cv2_imshow(image2)\n",
        "\n",
        "# Save the final image\n",
        "save_path = '/Users/prasadgole/Desktop/Illinois/Computational Photography/Group Project/CS445_Group_Project/images/outputs/final_composed_image.jpg'\n",
        "cv2.imwrite(save_path, image2)\n",
        "output_image = Image.open(save_path)\n",
        "\n",
        "plt.imshow(output_image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igEI0RkJ5aGA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load images\n",
        "image1 = cv2.imread('/content/drive/My Drive/room4.jpg')  # Load the coffee table image\n",
        "image2 = cv2.imread('/Users/prasadgole/Desktop/Illinois/Computational Photography/Group Project/CS445_Group_Project/Project_Files/room4.jpg')          # Load the room image\n",
        "\n",
        "# Assuming maskstable and bounding box information is correctly provided\n",
        "mask_coffee = maskstable[1]['segmentation']\n",
        "bbox_coffee = maskstable[1]['bbox']\n",
        "\n",
        "cropped_mask = mask_coffee[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "cropped_image = image1[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "\n",
        "scale_factor = 0.5\n",
        "resized_image = cv2.resize(cropped_image, (int(cropped_image.shape[1] * scale_factor), int(cropped_image.shape[0] * scale_factor)))\n",
        "resized_mask = cv2.resize(cropped_mask, (int(cropped_mask.shape[1] * scale_factor), int(cropped_mask.shape[0] * scale_factor)))\n",
        "\n",
        "table_extracted = cv2.bitwise_and(resized_image, resized_image, mask=resized_mask)\n",
        "\n",
        "start_x, start_y = 515, 140\n",
        "end_x = start_x + table_extracted.shape[1]\n",
        "end_y = start_y + table_extracted.shape[0]\n",
        "\n",
        "image2[start_y:end_y, start_x:end_x] = image2[start_y:end_y, start_x:end_x] * (1 - (resized_mask > 0)[:, :, None]) + table_extracted\n",
        "\n",
        "# Display the final image\n",
        "cv2_imshow(image2)\n",
        "\n",
        "# Save the final image to disk\n",
        "save_path = '/Users/prasadgole/Desktop/Illinois/Computational Photography/Group Project/CS445_Group_Project/images/outputs'\n",
        "cv2.imwrite(save_path, image2)\n",
        "print(\"Image saved successfully at:\", save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPrO42-7I_jH"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Load images\n",
        "image1 = cv2.imread('/content/drive/My Drive/better_sofa.jpg')  # Load the coffee table image\n",
        "image2 = cv2.imread('/content/drive/My Drive/final_output_image2.jpg')          # Load the room image\n",
        "\n",
        "mask_coffee = masks_couch10[2]['segmentation']\n",
        "bbox_coffee = masks_couch10[2]['bbox']\n",
        "\n",
        "cropped_mask = mask_coffee[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "cropped_image = image1[bbox_coffee[1]:bbox_coffee[1]+bbox_coffee[3], bbox_coffee[0]:bbox_coffee[0]+bbox_coffee[2]]\n",
        "\n",
        "cropped_mask = (cropped_mask > 0).astype('uint8') * 255\n",
        "\n",
        "scale_factor = 0.45\n",
        "resized_image = cv2.resize(cropped_image, (int(cropped_image.shape[1] * scale_factor), int(cropped_image.shape[0] * scale_factor)))\n",
        "resized_mask = cv2.resize(cropped_mask, (int(cropped_mask.shape[1] * scale_factor), int(cropped_mask.shape[0] * scale_factor)))\n",
        "\n",
        "table_extracted = cv2.bitwise_and(resized_image, resized_image, mask=resized_mask)\n",
        "\n",
        "start_x, start_y = 220, 220\n",
        "end_x = start_x + table_extracted.shape[1]\n",
        "end_y = start_y + table_extracted.shape[0]\n",
        "\n",
        "image2[start_y:end_y, start_x:end_x] = image2[start_y:end_y, start_x:end_x] * (1 - (resized_mask > 0)[:, :, None]) + table_extracted\n",
        "\n",
        "cv2_imshow(image2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow  # Import cv2_imshow to replace cv2.imshow in a Colab environment\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "import scipy.sparse.linalg\n",
        "\n",
        "def blender(object_img, object_mask, bg_img, bg_ul):\n",
        "    # Crop the object image and mask according to the bounding box\n",
        "    object_height, object_width, _ = object_img.shape\n",
        "    bg_height, bg_width, _ = bg_img.shape\n",
        "    ii, jj = np.nonzero(object_mask)\n",
        "    num_pixels = len(ii)\n",
        "    ii += bg_ul[0]  # Adjust indices based on the upper-left position\n",
        "    jj += bg_ul[1]\n",
        "\n",
        "    # Create the sparse matrix and vector for the linear system\n",
        "    A = scipy.sparse.lil_matrix((num_pixels, num_pixels))\n",
        "    b = np.zeros(num_pixels)\n",
        "    indices = {(i, j): idx for idx, (i, j) in enumerate(zip(ii, jj))}\n",
        "\n",
        "    # Fill the matrix A and vector b based on the neighbors\n",
        "    for idx, (i, j) in enumerate(zip(ii, jj)):\n",
        "        neighbors = [(i-1, j), (i+1, j), (i, j-1), (i, j+1)]\n",
        "        for ni, nj in neighbors:\n",
        "            if (ni, nj) in indices:\n",
        "                A[idx, indices[(ni, nj)]] = -1\n",
        "            A[idx, idx] += 1\n",
        "            if ni < 0 or nj < 0 or ni >= bg_height or nj >= bg_width:\n",
        "                pixel_value = bg_img[ni % bg_height, nj % bg_width]\n",
        "                b[idx] += np.mean(pixel_value)  # Use the average of RGB values\n",
        "            else:\n",
        "                if (ni, nj) not in indices:\n",
        "                    pixel_value = bg_img[ni, nj]\n",
        "                    b[idx] += np.mean(pixel_value)  # Use the average of RGB values\n",
        "\n",
        "    # Solve the linear system\n",
        "    x = scipy.sparse.linalg.lsqr(A, b)[0]\n",
        "\n",
        "    # Create the result image from the background\n",
        "    result_img = np.copy(bg_img)\n",
        "    for idx, val in enumerate(x):\n",
        "        i, j = ii[idx], jj[idx]\n",
        "        if 0 <= i < bg_height and 0 <= j < bg_width:\n",
        "            result_img[i, j] = val  # Assign the grayscale value to all channels\n",
        "\n",
        "    return result_img\n",
        "\n",
        "# Example usage\n",
        "\n",
        "\n",
        "\n",
        "image1 = cv2.imread('/content/drive/My Drive/room.jpg')\n",
        "image2 = cv2.imread('/content/drive/My Drive/room2.jpg')\n",
        "bbox = masks[1]['bbox']  # Bounding box from the masks data structure\n",
        "\n",
        "\n",
        "#[start_y:end_y, start_x:end_x]\n",
        "newbox = [start_y, start_x, end_y-start_y, end_x-start_x]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "result_image = blender(image1, cropped_mask, image2, newbox)\n",
        "\n",
        "# Display the result- our initial blending result wasn't able to come out perfectly, so we maintained the maintain the shape in the resemblence\n",
        "cv2_imshow(result_image)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "oeBlgfxOwmE2",
        "outputId": "a8c92b30-18bd-4b01-91de-e4cdde0d14b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'masks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0cbd1b899ccd>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mimage1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/room.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mimage2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/room2.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bbox'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Bounding box from the masks data structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'masks' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}